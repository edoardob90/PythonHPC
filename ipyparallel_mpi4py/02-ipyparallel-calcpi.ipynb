{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate $\\pi$ with IPython Parallel\n",
    "\n",
    "A simple Monte Carlo simulation to approximate the value of  $\\pi$  involves randomly selecting points in a unit square and determining how many of them land in $x^2+y^2=1$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the ipcluster in a terminal with 4 engines:\n",
    "```\n",
    "    $ source pythonhpc.sh \n",
    "    $ ipcluster start --n 4 &\n",
    "```\n",
    "Then import ipyparallel in your notebook, and initialize a Client instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ipyparallel.client.client.Client at 0x2aaab3f89d68>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipyparallel as ipp\n",
    "client = ipp.Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a DirectView object for direct execution on the engines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DirectView [0, 1, 2, 3,...]>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dview = client[:]\n",
    "dview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallelize the evaluation of $pi$ using a Monte Carlo method. First load modules, and export the random module to the engines: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "from math import pi\n",
    "dview['random'] = random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function monte_carlo_pi is a Monte Carlo method to calculate $\\pi$. We will time the execution of this function using %timeit -n 1 and a sample size of 10 million. This is running in serial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_pi(nsamples):\n",
    "    s = 0\n",
    "    for i in range(nsamples):\n",
    "        x = random()\n",
    "        y = random()\n",
    "        if x*x + y*y <= 1:\n",
    "            s+=1\n",
    "    return 4.*s/nsamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.141332\n",
      "3.1411688\n",
      "3.1413156\n",
      "3.1408932\n",
      "3.1422228\n",
      "3.1419028\n",
      "3.141344\n",
      "2.09 s ± 43.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 \n",
    "pi_mc = monte_carlo_pi(10000000)\n",
    "# should take a couple of seconds per timeit trial\n",
    "print(pi_mc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>Excercise</mark>: The incomplete function below should take a DirectView object and a number of samples, divide the number of samples between the engines, and call monte_carlo_pi() with a subset of the samples on each engine. \n",
    "\n",
    "Complete the parallel function (by replacing the \"TO_DO\" fields), call it with $10^7$ samples, time it and compare the performance with the serial version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dview.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_monte_carlo_pi(view, nsamples):\n",
    "    p = len(view.targets)\n",
    "    if nsamples % p:\n",
    "        # to ensure even divisibility\n",
    "        nsamples += p - (nsamples%p)\n",
    "    \n",
    "    subsamples = nsamples//p\n",
    "    \n",
    "    ar = view.apply(monte_carlo_pi, subsamples)\n",
    "    return sum(ar)/p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322 ms ± 5.39 ms per loop (mean ± std. dev. of 7 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 5\n",
    "multi_monte_carlo_pi(dview, 10000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>Question</mark> Confirm that the results of the serial and parallel pi calculation are correct.\n",
    "\n",
    "<mark>Question</mark> Did you see any speedup? \n",
    "\n",
    "Add another 2 engines by running \n",
    "\n",
    "```\n",
    "$ ipengine &\n",
    "```\n",
    "\n",
    "twice in your terminal. Ensure that your view includes the additional engines. Run the parallel $\\pi$ calculation again. Do you see any further speedup? \n",
    "\n",
    "<mark>Question</mark> Finally add another 2 engines and test again. Have you reached the limit of scalability on this toy problem?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miniconda-pythonhpc",
   "language": "python",
   "name": "miniconda-pythonhpc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
