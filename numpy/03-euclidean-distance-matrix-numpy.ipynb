{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the Euclidean Distance Matrix with NumPy\n",
    "\n",
    "In this notebook we implement two functions to compute the Euclidean distance matrix. We use a simple algebra trick that makes possible to write the function in a completely vectorized way in terms of optimized NumPy functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_broadcast(x, y):\n",
    "    \"\"\"Euclidean square distance matrix.\n",
    "    \n",
    "    Inputs:\n",
    "    x: (N, m) numpy array\n",
    "    y: (N, m) numpy array\n",
    "    \n",
    "    Ouput:\n",
    "    (N, N) Euclidean square distance matrix:\n",
    "    r_ij = x_ij^2 - y_ij^2\n",
    "    \"\"\"\n",
    "    diff = x[:, np.newaxis, :] - y[np.newaxis, :, :]\n",
    "\n",
    "    return (diff * diff).sum(axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark> Question </mark> At this point you are starting to get acquainted with the `numpy.ndarray`s and it's memory managment. Could you analyse the advantage and possible drawbacks of the `euclidean_broadcast` function? Write a positive and a negative point about it.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider now a more sophisticated implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_trick(x, y):\n",
    "    \"\"\"Euclidean square distance matrix.\n",
    "    \n",
    "    Inputs:\n",
    "    x: (N, m) numpy array\n",
    "    y: (N, m) numpy array\n",
    "    \n",
    "    Ouput:\n",
    "    (N, N) Euclidean square distance matrix:\n",
    "    r_ij = x_ij^2 - y_ij^2\n",
    "    \"\"\"\n",
    "    x2 = np.einsum('ij,ij->i', x, x)[:, np.newaxis]\n",
    "    y2 = np.einsum('ij,ij->i', y, y)[np.newaxis, :]\n",
    "\n",
    "    xy = np.dot(x, y.T)\n",
    "\n",
    "    return np.abs(x2 + y2 - 2. * xy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `euclidean_trick` function\n",
    "\n",
    "Each element of the Euclidean distance matrix is the scalar product of the diference between two rows fo the dataset. `euclidean_trick` takes advantage of this by doing the following\n",
    "$$\\sum_k {(x_{ik}-y_{jk})^2} = (\\vec{x}_i - \\vec{y}_j)\\cdot(\\vec{x}_i - \\vec{y}_j) = \\vec{x}_i\\cdot\\vec{x}_i + \\vec{y}_j\\cdot\\vec{y}_j - 2\\vec{x}_i\\cdot\\vec{y}_j$$\n",
    "\n",
    "There are NumPy functions to compute each of these terms:\n",
    "\n",
    "$\\vec{x}_i\\cdot\\vec{y}_j$ $\\rightarrow$ `np.dot(x, y)` : Matrix product of $\\{\\vec{x}\\}$ and $\\{\\vec{y}\\}$\n",
    "\n",
    "$\\vec{x}_i\\cdot\\vec{x}_i$ $\\rightarrow$ `np.einsum('ij,ij->i', x, x)[:, np.newaxis]` : A $(n,1)$ vector of elements $\\sum_j x_{ij}x_{ij}$\n",
    "\n",
    "$\\vec{y}_j\\cdot\\vec{y}_j$ $\\rightarrow$ `np.einsum('ij,ij->i', y, y)[np.newaxis, :]` : A $(1,n)$ vector of elements $\\sum_j y_{ij}y_{ij}$\n",
    "\n",
    "To have all the combinations $ij$ of the sum $\\vec{x}_i\\cdot\\vec{x}_i + \\vec{y}_j\\cdot\\vec{y}_j$, we add a new axis to each of the arrays, transpose one them and add them.\n",
    "\n",
    "Let's see now how the `np.einsum` function works. `einsum` stands for Einstein summation, which is used in tensor algebra to write compact expressions without the sum symbol ($\\sum$). Within the Einstein summation notation, whenever there are repeated indexes, there is a sum over them. For instance, the expression\n",
    "$$x_{ik}y_{kj}$$\n",
    "is equivalent to\n",
    "$$\\sum_k x_{ik}y_{kj}$$\n",
    "\n",
    "`np.einsum` uses a generalized form of the Einstein summation by adding the symbol `->` to prevent summing over certain indexes. The specific operation we use here, `np.einsum('ij,ij->i', x, x)`, gives the vector\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\sum_k x_{1k}x_{1k} \\\\\n",
    "\\sum_k x_{2k}x_{2k} \\\\\n",
    " ...                \\\\\n",
    "\\sum_k x_{nk}x_{nk} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Note that the resulting vector is represented here as a column vector just for visualization purposes. It's is `(n,)` NumPy array.\n",
    "\n",
    "Let's check now step-by-step what the `euclidean_trick` function does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets generate some random data\n",
    "nsamples = 10\n",
    "nfeat = 3\n",
    "\n",
    "x = 10. * np.random.random([nsamples, nfeat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.52413725, 0.81874071, 1.08225523],\n",
       "       [6.82255766, 5.56830485, 5.3885953 ],\n",
       "       [4.75472335, 9.49855868, 4.50573565],\n",
       "       [9.42894521, 2.9231334 , 6.75610111],\n",
       "       [4.65505557, 0.87968187, 8.78403851],\n",
       "       [7.35359595, 0.7664588 , 9.18966047],\n",
       "       [0.42007243, 9.49794582, 6.53669695],\n",
       "       [7.07057793, 8.90325152, 8.6998816 ],\n",
       "       [4.37518664, 9.73655124, 6.147031  ],\n",
       "       [7.22248158, 3.82786273, 5.73062056]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 58.45425409 106.59027131 133.13166497 143.0946188   99.60271518\n",
      " 139.11269194 133.11584265 204.94889989 151.72867842  99.6567853 ]\n"
     ]
    }
   ],
   "source": [
    "x2 = np.einsum('ij,ij->i', x, x)\n",
    "x2.shape\n",
    "print(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 58.45425409, 106.59027131, 133.13166497, 143.0946188 ,\n",
       "        99.60271518, 139.11269194, 133.11584265, 204.94889989,\n",
       "       151.72867842,  99.6567853 ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.52413725 0.81874071 1.08225523]\n",
      " [6.82255766 5.56830485 5.3885953 ]\n",
      " [4.75472335 9.49855868 4.50573565]\n",
      " [9.42894521 2.9231334  6.75610111]\n",
      " [4.65505557 0.87968187 8.78403851]\n",
      " [7.35359595 0.7664588  9.18966047]\n",
      " [0.42007243 9.49794582 6.53669695]\n",
      " [7.07057793 8.90325152 8.6998816 ]\n",
      " [4.37518664 9.73655124 6.147031  ]\n",
      " [7.22248158 3.82786273 5.73062056]]\n"
     ]
    }
   ],
   "source": [
    "x2 = np.einsum('ij,ij->i', x, x)[:, np.newaxis]\n",
    "x2.shape\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[116.90850818, 165.0445254 , 191.58591907, 201.54887289,\n",
       "        158.05696928, 197.56694603, 191.57009674, 263.40315398,\n",
       "        210.18293251, 158.11103939],\n",
       "       [165.0445254 , 213.18054261, 239.72193628, 249.6848901 ,\n",
       "        206.19298649, 245.70296325, 239.70611396, 311.5391712 ,\n",
       "        258.31894973, 206.24705661],\n",
       "       [191.58591907, 239.72193628, 266.26332995, 276.22628377,\n",
       "        232.73438016, 272.24435691, 266.24750763, 338.08056486,\n",
       "        284.8603434 , 232.78845028],\n",
       "       [201.54887289, 249.6848901 , 276.22628377, 286.1892376 ,\n",
       "        242.69733398, 282.20731074, 276.21046145, 348.04351869,\n",
       "        294.82329722, 242.7514041 ],\n",
       "       [158.05696928, 206.19298649, 232.73438016, 242.69733398,\n",
       "        199.20543037, 238.71540713, 232.71855784, 304.55161507,\n",
       "        251.33139361, 199.25950049],\n",
       "       [197.56694603, 245.70296325, 272.24435691, 282.20731074,\n",
       "        238.71540713, 278.22538388, 272.22853459, 344.06159183,\n",
       "        290.84137036, 238.76947724],\n",
       "       [191.57009674, 239.70611396, 266.24750763, 276.21046145,\n",
       "        232.71855784, 272.22853459, 266.23168531, 338.06474254,\n",
       "        284.84452107, 232.77262795],\n",
       "       [263.40315398, 311.5391712 , 338.08056486, 348.04351869,\n",
       "        304.55161507, 344.06159183, 338.06474254, 409.89779978,\n",
       "        356.67757831, 304.60568519],\n",
       "       [210.18293251, 258.31894973, 284.8603434 , 294.82329722,\n",
       "        251.33139361, 290.84137036, 284.84452107, 356.67757831,\n",
       "        303.45735684, 251.38546372],\n",
       "       [158.11103939, 206.24705661, 232.78845028, 242.7514041 ,\n",
       "        199.25950049, 238.76947724, 232.77262795, 304.60568519,\n",
       "        251.38546372, 199.3135706 ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x2 + x2.T)#.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use `np.dot` to perform the matrix multiplication of the full dataset by itself. We didn't use it before as alternative to `np.einsum` because it doesn't perform row by row scalar products. Instead `np.dot` expects two arrays with matching shapes $(m,n)$ and $(n,m)$ to perform a matrix multiplication.\n",
    "\n",
    "We could have used `np.einsum('ik,jk', x, x)` to perform the matrix multiplication, but we chose `np.dot(x, x.T)` instead. This is because `np.dot` is a very sophisticated too, plus it uses OpenMP threads. This results in a very fast execution.\n",
    "\n",
    "You are wellcome to time them and look at the `top` command to see how `np.dot` uses multiple OpenMP threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy = np.dot(x, x.T)\n",
    "xy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, considering that the reason we are using `np.einsum` is to get rid of the loops, why didn't we use something like `(x*x).sum(axis=1)`? Let's run the next cell comparing them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0913936421275139e-11\n",
      "131 µs ± 1.05 µs per loop (mean ± std. dev. of 5 runs, 10000 loops each)\n",
      "243 µs ± 71.6 ns per loop (mean ± std. dev. of 5 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# let's use a larger array for timing the function calls\n",
    "nsamples = 1000\n",
    "nfeat = 300\n",
    "\n",
    "x = 10. * np.random.random([nsamples, nfeat])\n",
    "\n",
    "# it gives the same result\n",
    "print(np.abs(np.einsum('ij,ij->i', x, x) - (x*x).sum(axis=1)).max())\n",
    "\n",
    "# but it's not as fast as `np.einsum`\n",
    "%timeit -n 10000 -r 5 np.einsum('ij,ij->i', x, x)\n",
    "%timeit -n 10000 -r 5 (x*x).sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing a reduction with the ufunc `np.add` is also slower than `np.einsum`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "490 µs ± 170 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.add.reduce(x*x, axis=1)\n",
    "\n",
    "# As homework, check what's the meaning of the previous line!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's time both implementations and check that they give the same result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "773 ms ± 5.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "36.9 ms ± 539 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "nsamples = 2000\n",
    "nfeat = 50\n",
    "\n",
    "x = 10. * np.random.random([nsamples, nfeat])\n",
    "\n",
    "%timeit euclidean_broadcast(x, x)\n",
    "%timeit euclidean_trick(x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.296918293926865e-12"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(euclidean_broadcast(x, x) - euclidean_trick(x, x)).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark> Optional question </mark> Change the implementation of `euclidean_broadcast` function to make faster using `einsum` to do the final sum. How much is the speed up? Compare it with both the original `euclidean_broadcast` and `euclidean_trick`. Check that the result is the same!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/edm-broadcast-einsum.py\n",
    "idean_broadcast(x, y):\n",
    "    \"\"\"Euclidean square distance matrix.\n",
    "    \n",
    "    Inputs:\n",
    "    x: (N,) numpy array\n",
    "    y: (N,) numpy array\n",
    "    \n",
    "    Ouput:\n",
    "    (N, N) Euclidean square distance matrix:\n",
    "    r_ij = x_ij^2 - y_ij^2\n",
    "    \"\"\"\n",
    "    diff = x[:, np.newaxis, :] - y[np.newaxis, :, :]\n",
    "\n",
    "    return np.einsum('ijk,ijk->ij', diff, diff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "The main points to take from this notebook are:\n",
    "  * NumPy is all about vectorization. Loops in python must be avoided.\n",
    "  * Always consider different vectorized implementations and compare them.\n",
    "  * Even within NumPy, some functions might bring a more significant speedup than others."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miniconda-pythonhpc",
   "language": "python",
   "name": "miniconda-pythonhpc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
